"""
KoSimCSE ê¸°ë°˜ í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© í´ë˜ìŠ¤
BM-K/KoSimCSE-roberta ëª¨ë¸ì„ LangChain Embeddings ì¸í„°í˜ì´ìŠ¤ì— ë§ê²Œ ë˜í•‘
"""
import torch
from typing import List
from transformers import AutoModel, AutoTokenizer
from langchain_core.embeddings import Embeddings


class KoSimCSEEmbeddings(Embeddings):
    """KoSimCSE ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” í•œêµ­ì–´ ì„ë² ë”© í´ë˜ìŠ¤"""

    def __init__(self, model_name: str = "BM-K/KoSimCSE-roberta", device: str = None):
        """
        KoSimCSE ì„ë² ë”© ì´ˆê¸°í™”

        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸: BM-K/KoSimCSE-roberta)
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (ê¸°ë³¸: auto-detect)
        """
        self.model_name = model_name

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        print(f"ğŸ¤– KoSimCSE ëª¨ë¸ ë¡œë”© ì¤‘... (device: {self.device})")

        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.model = AutoModel.from_pretrained(self.model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.model.to(self.device)
        self.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •

        print("âœ… KoSimCSE ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ (ê° ë²¡í„°ëŠ” 768ì°¨ì›)
        """
        if not texts:
            return []

        # í† í°í™”
        inputs = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=512  # KoSimCSE ê¶Œì¥ ìµœëŒ€ ê¸¸ì´
        )

        # ì…ë ¥ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # ì„ë² ë”© ì¶”ì¶œ (gradient ê³„ì‚° ë¹„í™œì„±í™”)
        with torch.no_grad():
            outputs = self.model(**inputs)
            # [CLS] í† í°ì˜ ì„ë² ë”©ì„ ì‚¬ìš© (ì²« ë²ˆì§¸ í† í°)
            embeddings = outputs.last_hidden_state[:, 0, :]

        # CPUë¡œ ì´ë™í•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        embeddings = embeddings.cpu().numpy().tolist()

        return embeddings

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (LangChain ì¸í„°í˜ì´ìŠ¤)

        Args:
            texts: ì„ë² ë”©í•  ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        return self._get_embeddings(texts)

    def embed_query(self, text: str) -> List[float]:
        """
        ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (LangChain ì¸í„°í˜ì´ìŠ¤)

        Args:
            text: ì„ë² ë”©í•  ì¿¼ë¦¬ í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        embeddings = self._get_embeddings([text])
        return embeddings[0] if embeddings else []

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """
        ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì°¸ì¡° ì½”ë“œì˜ cal_score ê¸°ëŠ¥)

        Args:
            text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸

        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0-100 ë²”ìœ„)
        """
        embeddings = self._get_embeddings([text1, text2])
        if len(embeddings) != 2:
            return 0.0

        # í…ì„œë¡œ ë³€í™˜
        a = torch.tensor(embeddings[0])
        b = torch.tensor(embeddings[1])

        # ì •ê·œí™”
        a_norm = a / a.norm()
        b_norm = b / b.norm()

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (0-100 ìŠ¤ì¼€ì¼)
        similarity = torch.dot(a_norm, b_norm) * 100

        return similarity.item()


def test_kosimcse():
    """KoSimCSE ì„ë² ë”© í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª KoSimCSE ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹œì‘...")

    # ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    embeddings = KoSimCSEEmbeddings()

    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤ (ì°¸ì¡° ì½”ë“œì™€ ë™ì¼)
    sentences = [
        'ì¹˜íƒ€ê°€ ë“¤íŒì„ ê°€ë¡œ ì§ˆëŸ¬ ë¨¹ì´ë¥¼ ì«“ëŠ”ë‹¤.',
        'ì¹˜íƒ€ í•œ ë§ˆë¦¬ê°€ ë¨¹ì´ ë’¤ì—ì„œ ë‹¬ë¦¬ê³  ìˆë‹¤.',
        'ì›ìˆ­ì´ í•œ ë§ˆë¦¬ê°€ ë“œëŸ¼ì„ ì—°ì£¼í•œë‹¤.'
    ]

    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë¬¸ì¥: {sentences}")

    # ì„ë² ë”© ë²¡í„° ìƒì„±
    doc_embeddings = embeddings.embed_documents(sentences)
    print(f"ğŸ“Š ì„ë² ë”© ì°¨ì›: {len(doc_embeddings[0])}")

    # ìœ ì‚¬ë„ ê³„ì‚°
    score01 = embeddings.calculate_similarity(sentences[0], sentences[1])
    score02 = embeddings.calculate_similarity(sentences[0], sentences[2])

    print(f"ğŸ” ìœ ì‚¬ë„ ê²°ê³¼:")
    print(f"  '{sentences[0]}' vs '{sentences[1]}': {score01:.2f}")
    print(f"  '{sentences[0]}' vs '{sentences[2]}': {score02:.2f}")

    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_kosimcse()