import frontmatter
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing import List, Dict, Any
import re


def clean_text(text: str) -> str:
    """ì„œë¡œê²Œì´íŠ¸ ì—ëŸ¬ ì™„ì „ í•´ê²°í•˜ëŠ” í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not text:
        return ""
    
    try:
        # 1ë‹¨ê³„: ë¬¸ì ë‹¨ìœ„ë¡œ ì„œë¡œê²Œì´íŠ¸ ì œê±°
        cleaned_chars = []
        for char in text:
            try:
                # ì„œë¡œê²Œì´íŠ¸ ë²”ìœ„ í™•ì¸ (0xD800-0xDFFF)
                code_point = ord(char)
                if 0xD800 <= code_point <= 0xDFFF:
                    # ì„œë¡œê²Œì´íŠ¸ ë¬¸ìëŠ” ìŠ¤í‚µ
                    continue
                    
                # UTF-8ë¡œ ì¸ì½”ë”©/ë””ì½”ë”© ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
                char.encode('utf-8').decode('utf-8')
                cleaned_chars.append(char)
                
            except (UnicodeError, ValueError):
                # ë¬¸ì œ ìˆëŠ” ë¬¸ìëŠ” ìŠ¤í‚µ
                continue
        
        cleaned_text = ''.join(cleaned_chars)
        
        # 2ë‹¨ê³„: ì „ì²´ í…ìŠ¤íŠ¸ ë‹¤ì‹œ ì •ë¦¬
        cleaned_text = cleaned_text.encode('utf-8', 'ignore').decode('utf-8')
        
        # 3ë‹¨ê³„: ì œì–´ ë¬¸ì ë° ê³µë°± ì •ë¦¬
        cleaned_text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', cleaned_text)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        
        return cleaned_text.strip()
        
    except Exception as e:
        print(f"âš ï¸ ì¹˜ëª…ì  í…ìŠ¤íŠ¸ ì •ë¦¬ ì—ëŸ¬: {e}")
        # ìµœí›„ì˜ ìˆ˜ë‹¨: ì•„ìŠ¤í‚¤ë§Œ ë‚¨ê¸°ê¸°
        return ''.join(char for char in text if ord(char) < 128).strip()


def create_text_splitter(chunk_size: int = 1000, chunk_overlap: int = 200):
    """í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ìƒì„±"""
    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", "? ", "! ", " ", ""],
    )


def parse_markdown_file(file_path: Path) -> Dict[str, Any]:
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ í•˜ë‚˜ íŒŒì‹±"""
    with open(file_path, "r", encoding="utf-8") as f:
        post = frontmatter.load(f)

    return {
        "content": clean_text(post.content),  # í…ìŠ¤íŠ¸ ì •ë¦¬
        "metadata": {
            "source": str(file_path),
            "title": clean_text(post.metadata.get("title", file_path.stem)),  # ì œëª©ë„ ì •ë¦¬
            "tags": ", ".join(post.metadata.get("tags", [])),  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            "date": str(post.metadata.get("date", "")),  # Noneì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
            "file_name": file_path.name,
        },
    }


def chunk_text(content: str, text_splitter) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    return text_splitter.split_text(content) if content else []


def create_document_chunks(
    parsed_doc: Dict[str, Any], text_splitter
) -> List[Dict[str, Any]]:
    """íŒŒì‹±ëœ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë³€í™˜"""
    chunks = chunk_text(parsed_doc["content"], text_splitter)

    document_chunks = []
    for i, chunk in enumerate(chunks):
        chunk_metadata = parsed_doc["metadata"].copy()
        chunk_metadata.update({"chunk_index": i, "total_chunks": len(chunks)})

        document_chunks.append({"content": chunk, "metadata": chunk_metadata})

    return document_chunks


def process_obsidian_vault(
    vault_path: str, chunk_size: int = 1000, chunk_overlap: int = 200
) -> List[Dict[str, Any]]:
    """ì˜µì‹œë””ì–¸ ë³¼íŠ¸ ì „ì²´ ì²˜ë¦¬"""
    vault_path = Path(vault_path)
    text_splitter = create_text_splitter(chunk_size, chunk_overlap)
    all_chunks = []

    for md_file in vault_path.rglob("*.md"):
        print(f"ğŸ“– ì²˜ë¦¬ ì¤‘: {md_file.name}")

        try:
            parsed_doc = parse_markdown_file(md_file)
            chunks = create_document_chunks(parsed_doc, text_splitter)
            all_chunks.extend(chunks)
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ {md_file}: {e}")

    print(f"âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")
    return all_chunks
